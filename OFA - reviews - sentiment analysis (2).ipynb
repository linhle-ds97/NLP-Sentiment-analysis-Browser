{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818abf4c-cd1a-487b-8747-56a00adce227",
   "metadata": {},
   "source": [
    "# PROJECT: SENTIMENT ANALYSIS - PRODUCT: OFA\n",
    "\n",
    "Author: Linh Le / dieulinh97.bi@gmail.com/ Oslo, Norway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf32544-70d8-46b3-8234-34e5ec6d4b6c",
   "metadata": {},
   "source": [
    "Script description: This script is to create a complete table with 3 most important attributes, which are main_word( top commonly mentioned nouns/features in the reviews of users), sub_word (top commonly mentioned adjectives/attitudes in the reviews of users), star_rating(score given for each reviews by users).\n",
    "\n",
    "- The outcome is top common features, top common adjectives for each feature, average star rating overtime for each feature. The tables are ready for the use of creating dashboard in Looker.\n",
    "\n",
    "The script has the following steps:\n",
    "1. Extract all English reviews from BigQuery review datasets. The reason is because there are a lot of reviews written in English while the reviewerLanguages are recorded in other languages rather than English. \n",
    "2. After having the complete English review datasets, we use NLP to process text, then count the word frequency to find the most commonly mentioned nouns/features. Save the complete table in BigQuery. (Temporarily called : 'Main Word' table)\n",
    "3. Once having the 'Main Word' table, we use it to create the 'Sub Word' table, in which sub-words are top commmonly used adjectives in the reviews containing main words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53db16b4-83c4-4c55-a400-bd0d890cd163",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-26T20:24:20.711768Z",
     "iopub.status.busy": "2023-03-26T20:24:20.710763Z",
     "iopub.status.idle": "2023-03-26T20:24:45.847019Z",
     "shell.execute_reply": "2023-03-26T20:24:45.845312Z",
     "shell.execute_reply.started": "2023-03-26T20:24:20.711693Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pube-public-vault:****@nexus.osp.opera.software/repository/pypi/simple\n",
      "Collecting nltk\n",
      "  Downloading https://nexus.osp.opera.software/repository/pypi/packages/nltk/3.8.1/nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading https://nexus.osp.opera.software/repository/pypi/packages/regex/2023.3.23/regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Installing collected packages: regex, nltk\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nltk-3.8.1 regex-2023.3.23\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pube-public-vault:****@nexus.osp.opera.software/repository/pypi/simple\n",
      "Requirement already satisfied: pip in /usr/local/share/miniconda3/lib/python3.8/site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Downloading https://nexus.osp.opera.software/repository/pypi/packages/pip/23.0.1/pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.8 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pip-23.0.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pube-public-vault:****@nexus.osp.opera.software/repository/pypi/simple\n",
      "Requirement already satisfied: seaborn in /usr/local/share/miniconda3/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from seaborn) (1.22.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (5.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pube-public-vault:****@nexus.osp.opera.software/repository/pypi/simple\n",
      "Collecting langdetect\n",
      "  Downloading https://nexus.osp.opera.software/repository/pypi/packages/langdetect/1.0.9/langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/share/miniconda3/lib/python3.8/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=4b3c505e8c5eadd736fbfbc98c04f68221b19583f0eb05d4edbace087c3bb697\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/aa/2b/96/79b1cb82975323388878037ca722806a230e23057b051c42d2\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pube-public-vault:****@nexus.osp.opera.software/repository/pypi/simple\n",
      "Collecting textblob\n",
      "  Downloading https://nexus.osp.opera.software/repository/pypi/packages/textblob/0.17.1/textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/jovyan/.local/lib/python3.8/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jovyan/.local/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2023.3.23)\n",
      "Requirement already satisfied: joblib in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# # Install essential modules\n",
    "# !pip install nltk\n",
    "# !pip install --upgrade pip\n",
    "# !pip install seaborn\n",
    "# !pip install langdetect\n",
    "# !pip install -U textblob\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a246b5a-40ed-47e6-a69e-3b7846efdd3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T09:09:10.897168Z",
     "iopub.status.busy": "2023-03-27T09:09:10.896174Z",
     "iopub.status.idle": "2023-03-27T09:09:11.671264Z",
     "shell.execute_reply": "2023-03-27T09:09:11.670013Z",
     "shell.execute_reply.started": "2023-03-27T09:09:10.897094Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88cca4-7bdc-4b4a-873e-1fd307230690",
   "metadata": {},
   "source": [
    "STEP 1: DATA PREPARATION\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b291d7a-2631-4ed6-a2f2-80537c0ecf17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T09:09:14.578538Z",
     "iopub.status.busy": "2023-03-27T09:09:14.577898Z",
     "iopub.status.idle": "2023-03-27T09:09:37.546316Z",
     "shell.execute_reply": "2023-03-27T09:09:37.544940Z",
     "shell.execute_reply.started": "2023-03-27T09:09:14.578514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(430283, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda3/lib/python3.8/site-packages/google/cloud/bigquery/table.py:2014: FutureWarning: Using .astype to convert from timezone-aware dtype to timezone-naive dtype is deprecated and will raise in a future version.  Use obj.tz_localize(None) or obj.tz_convert('UTC').tz_localize(None) instead\n",
      "  df[column] = pandas.Series(df[column], dtype=dtypes[column])\n"
     ]
    }
   ],
   "source": [
    "# Get the english reviews \n",
    "sql =\"\"\"\n",
    "SELECT date, text, starRating, reviewerLanguage, appVersionName\n",
    "FROM `osp-bu-mobile.google_play.ofa_reviews` \n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "df = pd.read_gbq(sql, project_id='osp-bu-mobile')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c497e6e4-dbd7-4976-baff-f887dbd43f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T09:09:47.218439Z",
     "iopub.status.busy": "2023-03-27T09:09:47.217545Z",
     "iopub.status.idle": "2023-03-27T09:09:47.226204Z",
     "shell.execute_reply": "2023-03-27T09:09:47.224606Z",
     "shell.execute_reply.started": "2023-03-27T09:09:47.218398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to get all english reviews from df_non\n",
    "\n",
    "def detect_english(text):\n",
    "    try:\n",
    "        detected_lang = detect(text)\n",
    "        if detected_lang == 'en':\n",
    "            return text\n",
    "        else:\n",
    "            return \"None\"\n",
    "    except:\n",
    "        detected_lang = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b5f840-80c5-4edf-af6f-fe7821ec9b53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T09:09:50.085938Z",
     "iopub.status.busy": "2023-03-27T09:09:50.084904Z",
     "iopub.status.idle": "2023-03-27T09:09:50.102670Z",
     "shell.execute_reply": "2023-03-27T09:09:50.100721Z",
     "shell.execute_reply.started": "2023-03-27T09:09:50.085860Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEXT PROCESSING\n",
    "# Remove numbers\n",
    "def remove_number(text):\n",
    "    clean_text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
    "    return clean_text\n",
    "\n",
    "# Remove special character\n",
    "import re\n",
    "def remove_character(text):\n",
    "    normal_string = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    return normal_string\n",
    "\n",
    "# Remove emojis\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1823ed-b794-4855-9cf4-bad6f47cd5db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T09:09:52.777158Z",
     "iopub.status.busy": "2023-03-27T09:09:52.776590Z",
     "iopub.status.idle": "2023-03-27T09:10:01.183382Z",
     "shell.execute_reply": "2023-03-27T09:10:01.181710Z",
     "shell.execute_reply.started": "2023-03-27T09:09:52.777085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply removing fucntions to the dataframe\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_emojis(x))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_number(x))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_character(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a718d94-2302-4ddb-adb8-e744a0ba61a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T09:10:06.311279Z",
     "iopub.status.busy": "2023-03-27T09:10:06.310768Z",
     "iopub.status.idle": "2023-03-27T10:09:06.045288Z",
     "shell.execute_reply": "2023-03-27T10:09:06.043812Z",
     "shell.execute_reply.started": "2023-03-27T09:10:06.311254Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The original shape of df_non:  (430283, 5)\n"
     ]
    }
   ],
   "source": [
    "# Apply detecting English review function to the dataframe (Note: this line of code takes quite a long time to run ~ 15 mins)\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: detect_english(x))\n",
    "print(\" The original shape of df_non: \", df.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6301bb-f678-4129-b577-abf376109b16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T10:40:58.407627Z",
     "iopub.status.busy": "2023-03-27T10:40:58.406589Z",
     "iopub.status.idle": "2023-03-27T10:40:58.572430Z",
     "shell.execute_reply": "2023-03-27T10:40:58.571166Z",
     "shell.execute_reply.started": "2023-03-27T10:40:58.407553Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of df after removing non-english reviews:  (259268, 5)\n"
     ]
    }
   ],
   "source": [
    "# Only take the satisfying reviews\n",
    "df = df[df['text'] != \"None\"]\n",
    "print(\"The shape of df after removing non-english reviews: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6158562-24c5-4150-9dab-2bbc917e5def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T10:41:03.568793Z",
     "iopub.status.busy": "2023-03-27T10:41:03.567812Z",
     "iopub.status.idle": "2023-03-27T10:41:04.934769Z",
     "shell.execute_reply": "2023-03-27T10:41:04.933392Z",
     "shell.execute_reply.started": "2023-03-27T10:41:03.568719Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an ID column for each review of the whole dataset\n",
    "df['ID'] = df.index + 1\n",
    "first_column = df.pop(\"ID\")\n",
    "df.insert(0, \"ID\", first_column)\n",
    "\n",
    "# Trunct month in date\n",
    "df['Month'] = df['date'].dt.strftime('%Y-%m')\n",
    "move_col = df.pop(\"Month\")\n",
    "df.insert(1,\"Month\", move_col)\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Replacing missing text with \"None\" and missing appVersionName with 0\n",
    "df['text'] = df['text'].fillna(\"None\")\n",
    "df['appVersionName'] = df['appVersionName'].fillna(0)\n",
    "\n",
    "# Make a copy of df\n",
    "df_copy = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412f70f-0524-4245-ac77-895262c7dff0",
   "metadata": {},
   "source": [
    "STEP 2: Figure out the most common keywords/ topics\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc298644-6ed2-410b-8e22-b1cb8eb54973",
   "metadata": {},
   "source": [
    "A.TEXT PROCESSING\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f225acc-a50a-499a-b9f2-ac839ea477af",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-27T10:41:11.777004Z",
     "iopub.status.busy": "2023-03-27T10:41:11.775975Z",
     "iopub.status.idle": "2023-03-27T10:41:19.964865Z",
     "shell.execute_reply": "2023-03-27T10:41:19.963334Z",
     "shell.execute_reply.started": "2023-03-27T10:41:11.776927Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pube-public-vault:****@nexus.osp.opera.software/repository/pypi/simple\n",
      "Requirement already satisfied: wordcloud in /home/jovyan/.local/lib/python3.8/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: pillow in /usr/local/share/miniconda3/lib/python3.8/site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/share/miniconda3/lib/python3.8/site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from wordcloud) (1.22.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (5.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (4.39.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->wordcloud) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/share/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pube-public-vault:****@nexus.osp.opera.software/repository/pypi/simple\n",
      "Requirement already satisfied: textblob in /home/jovyan/.local/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/jovyan/.local/lib/python3.8/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jovyan/.local/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2023.3.23)\n",
      "Requirement already satisfied: joblib in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/share/miniconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "\n",
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5591adbe-7b66-4b84-b19f-81118e7a9a79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T10:41:27.516978Z",
     "iopub.status.busy": "2023-03-27T10:41:27.515943Z",
     "iopub.status.idle": "2023-03-27T10:41:27.632596Z",
     "shell.execute_reply": "2023-03-27T10:41:27.631178Z",
     "shell.execute_reply.started": "2023-03-27T10:41:27.516905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "from os import path\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# from nltk.probability import FreDist\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from textblob import TextBlob\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa9e3d4b-ded1-4260-92ad-66330f81494d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T10:41:38.130101Z",
     "iopub.status.busy": "2023-03-27T10:41:38.129566Z",
     "iopub.status.idle": "2023-03-27T10:42:58.144960Z",
     "shell.execute_reply": "2023-03-27T10:42:58.143246Z",
     "shell.execute_reply.started": "2023-03-27T10:41:38.130075Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert all the text to lower strings\n",
    "df_copy['text'] = df_copy['text'].str.lower()\n",
    "\n",
    "# Tokenization\n",
    "df_copy['text'].apply(lambda x: TextBlob(str(x)).words).head()  # This line of code kinda takes time\n",
    "\n",
    "# Lemmatization: to break down the word to its root\n",
    "df_copy['text'] = df_copy['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in str(x).split()]))\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = stopwords.words(\"english\") # 179 stopwords for english\n",
    "df_copy['text'] = df_copy['text'].apply(lambda x : \" \".join(x for x in str(x).split() if x not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b230109c-3e7d-4bb5-9557-af48988a01b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T10:45:01.087307Z",
     "iopub.status.busy": "2023-03-27T10:45:01.086795Z",
     "iopub.status.idle": "2023-03-27T10:47:44.692387Z",
     "shell.execute_reply": "2023-03-27T10:47:44.690771Z",
     "shell.execute_reply.started": "2023-03-27T10:45:01.087282Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to get nouns only from the reviews\n",
    "def nouns_only(sentence):\n",
    "    noun_tag = ['NN','NNP','NNS','NNPS']\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    word_list = [word for word,pos in tag if (pos in noun_tag) if(word not in ['opera','browser','app','application','thank','work','use'])]\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "# Apply the nouns_only functions to all the text in the dataframe\n",
    "df_copy['text'] = df_copy['text'].apply(lambda x : nouns_only(x))    # Note: this line of code takes a bit long time to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c01e86-3b62-4fbb-9855-7c63ed561867",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T12:48:52.501848Z",
     "iopub.status.busy": "2023-01-25T12:48:52.501354Z",
     "iopub.status.idle": "2023-01-25T12:48:52.508781Z",
     "shell.execute_reply": "2023-01-25T12:48:52.507456Z",
     "shell.execute_reply.started": "2023-01-25T12:48:52.501821Z"
    },
    "tags": []
   },
   "source": [
    "B.TERM FREQUENCY\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e57c5ecc-182c-4d60-ba92-b0c34cd4005a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T10:48:16.670122Z",
     "iopub.status.busy": "2023-03-27T10:48:16.669042Z",
     "iopub.status.idle": "2023-03-27T11:01:34.639611Z",
     "shell.execute_reply": "2023-03-27T11:01:34.638633Z",
     "shell.execute_reply.started": "2023-03-27T10:48:16.670043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counting the term frequency to figure out the most common features mentioned in the reviews\n",
    "\n",
    "TF = df_copy[\"text\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index() # Note: this line of code takes ~ 25 mins\n",
    "TF.columns = [\"Word\", \"TF\"]\n",
    "\n",
    "# Sorting to get the most common words\n",
    "TF = TF.sort_values(by = \"TF\", ascending = False)\n",
    "\n",
    "# Getting 40 top common words\n",
    "words = TF['Word'].head(60).reset_index(drop = True)\n",
    "words = words.values.tolist()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38b99772-0df6-4630-ae31-df4b62359f24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T11:05:35.612142Z",
     "iopub.status.busy": "2023-03-27T11:05:35.611540Z",
     "iopub.status.idle": "2023-03-27T11:05:35.617914Z",
     "shell.execute_reply": "2023-03-27T11:05:35.617163Z",
     "shell.execute_reply.started": "2023-03-27T11:05:35.612104Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing some meaningless words in the top 40\n",
    "remove_val = ['','ha','wa','doe','dont','None','cant','cool','lot','im','doesnt','thanks','cool','love','pc','way','nothing','day','perfect','fast','none','convenient']\n",
    "for i in remove_val:\n",
    "    for word in words:\n",
    "        if word == i:\n",
    "            words.remove(i)\n",
    "            \n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3e1159-844e-40b7-895f-0b4d6b53a2d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T11:05:54.052782Z",
     "iopub.status.busy": "2023-03-27T11:05:54.051359Z",
     "iopub.status.idle": "2023-03-27T11:05:54.063417Z",
     "shell.execute_reply": "2023-03-27T11:05:54.061409Z",
     "shell.execute_reply.started": "2023-03-27T11:05:54.052705Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Adding new key words that Marketing team is considering\n",
    "words.extend(['chacoalhe','concorra','shake and win','shake & win', 'shake&win','speed dial'])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a60ec1c8-b6d4-4d3e-bfd8-a3144aa57e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T12:07:27.286720Z",
     "iopub.status.busy": "2023-03-27T12:07:27.286011Z",
     "iopub.status.idle": "2023-03-27T12:11:00.012523Z",
     "shell.execute_reply": "2023-03-27T12:11:00.011094Z",
     "shell.execute_reply.started": "2023-03-27T12:07:27.286694Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Word</th>\n",
       "      <th>StarRating</th>\n",
       "      <th>Country</th>\n",
       "      <th>AppVersion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-06-22 12:08:32+00:00</td>\n",
       "      <td>ad</td>\n",
       "      <td>5</td>\n",
       "      <td>AR</td>\n",
       "      <td>58.2.2878.53403</td>\n",
       "      <td>This is my favorite browser but please adjust ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-06-22 12:44:24+00:00</td>\n",
       "      <td>ad</td>\n",
       "      <td>5</td>\n",
       "      <td>PT</td>\n",
       "      <td>58.2.2878.53403</td>\n",
       "      <td>Best Browser Blocks all ads Something that in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-06-22 12:48:54+00:00</td>\n",
       "      <td>ad</td>\n",
       "      <td>2</td>\n",
       "      <td>RU</td>\n",
       "      <td>58.2.2878.53403</td>\n",
       "      <td>The application is bad it eats a lot of traffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-06-22 13:26:56+00:00</td>\n",
       "      <td>ad</td>\n",
       "      <td>5</td>\n",
       "      <td>DE</td>\n",
       "      <td>58.2.2878.53403</td>\n",
       "      <td>The app is really good the add blocker brings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-06-22 14:02:23+00:00</td>\n",
       "      <td>ad</td>\n",
       "      <td>5</td>\n",
       "      <td>EN</td>\n",
       "      <td>58.2.2878.53403</td>\n",
       "      <td>Great to get rid of annoying ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-06-22 14:14:22+00:00</td>\n",
       "      <td>ad</td>\n",
       "      <td>4</td>\n",
       "      <td>VI</td>\n",
       "      <td>nan</td>\n",
       "      <td>Block ads well but many pages automatically o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date Word  StarRating Country       AppVersion  \\\n",
       "0 2020-06-22 12:08:32+00:00   ad           5      AR  58.2.2878.53403   \n",
       "1 2020-06-22 12:44:24+00:00   ad           5      PT  58.2.2878.53403   \n",
       "2 2020-06-22 12:48:54+00:00   ad           2      RU  58.2.2878.53403   \n",
       "3 2020-06-22 13:26:56+00:00   ad           5      DE  58.2.2878.53403   \n",
       "4 2020-06-22 14:02:23+00:00   ad           5      EN  58.2.2878.53403   \n",
       "5 2020-06-22 14:14:22+00:00   ad           4      VI              nan   \n",
       "\n",
       "                                                Text  \n",
       "0  This is my favorite browser but please adjust ...  \n",
       "1  Best Browser Blocks all ads Something that in ...  \n",
       "2  The application is bad it eats a lot of traffi...  \n",
       "3  The app is really good the add blocker brings ...  \n",
       "4                   Great to get rid of annoying ads  \n",
       "5   Block ads well but many pages automatically o...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting values for keys in my_dict\n",
    "date_col = []\n",
    "word_col = []\n",
    "text_col = []\n",
    "score_col = []\n",
    "lang_col = []\n",
    "version_col = []\n",
    "\n",
    "for word in words:\n",
    "    for text in df['text']:\n",
    "        if word in text.lower():\n",
    "            word_col.append(word)\n",
    "            text_col.append(text)\n",
    "        else:\n",
    "            word_col.append(\"None\")\n",
    "            text_col.append(\"None\")\n",
    "    \n",
    "    for i in range(len(df['date'])):\n",
    "        date_col.append(df['date'][i])\n",
    "    \n",
    "    for score_line in df['starRating']:\n",
    "        score_col.append(score_line)\n",
    "    \n",
    "    for language in df['reviewerLanguage']:\n",
    "        lang_col.append(language)\n",
    "        \n",
    "    for version in df['appVersionName']:\n",
    "        version_col.append(version)\n",
    "        \n",
    "# Save all the required columns in a dictionary\n",
    "my_dict = {\"Date\" : date_col,\n",
    "           \"Word\" : word_col,\n",
    "           \"StarRating\" : score_col,\n",
    "           \"Country\" : lang_col,\n",
    "           \"AppVersion\" : version_col,\n",
    "           \"Text\" : text_col\n",
    "         }\n",
    "\n",
    "my_dict = pd.DataFrame(my_dict)\n",
    "\n",
    "# Columns processing\n",
    "my_dict = my_dict[my_dict['Word']!= \"None\"].reset_index(drop = True)\n",
    "my_dict['Country'] = my_dict['Country'].str.upper()\n",
    "\n",
    "print(my_dict.shape)\n",
    "my_dict.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8e6bbd1-78d5-4237-9dcb-f9a9c35f6db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T13:29:58.075906Z",
     "iopub.status.busy": "2023-03-27T13:29:58.074842Z",
     "iopub.status.idle": "2023-03-27T13:30:10.481635Z",
     "shell.execute_reply": "2023-03-27T13:30:10.480388Z",
     "shell.execute_reply.started": "2023-03-27T13:29:58.075881Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3429/770374911.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  my_dict.to_gbq(\n",
      "1it [00:11, 11.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Save a df to BQ\n",
    "my_dict.to_gbq(\n",
    "    \"bi_playground.sentiment_analysis_table_date_xxxyyy\", \"osp-bu-mobile\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdaea6-620f-41d4-89a1-54f88860a808",
   "metadata": {},
   "source": [
    "# STEP 3: CREATE SUB-WORDS TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2d80b-2b9c-4a7f-8134-dc14127686f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM `osp-bu-mobile.bi_playground.sentiment_analysis_table_date_xxxyyy` \n",
    "WHERE LOWER(Word) NOT IN ('none','developer','please','thing','everything','recommend','something', 'year','blocker','blocking')\n",
    "ORDER BY 1,2\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_gbq(sql, project_id='osp-bu-mobile')\n",
    "print(\" The original shape of df:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404425e-f12e-4734-ac0f-f9b819dd0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert ID column for each main_word row\n",
    "\n",
    "df['ID'] = df.index\n",
    "drop_col = df.pop('ID')\n",
    "df.insert(0,'ID', drop_col)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fa403-9baf-424f-9443-45a4d5d6a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Word list\n",
    "words = df['Word'].unique()\n",
    "words = words.tolist()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993379b-cf27-4191-b8c2-762a237841ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the adjtives only from the reviews in the Main Word table\n",
    "def adj_only(sentence):\n",
    "    adj_tags = [\"JJ\",\"JJR\",\"JJS\"]\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    word_list = [word for word,pos in tag if (pos in adj_tags)]\n",
    "    return ' '.join(word_list)\n",
    "                 \n",
    "\n",
    "# Define a function for NLP task\n",
    "\n",
    "def nlp_adj(dataframe):\n",
    "    # Convert all the text to lower strings\n",
    "    dataframe['Text'] = dataframe['Text'].str.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    dataframe['Text'].apply(lambda x: TextBlob(str(x)).words).head()  \n",
    "\n",
    "    # Lemmatization: to break down the word to its root\n",
    "    dataframe['Text'] = dataframe['Text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in str(x).split()]))\n",
    "\n",
    "    # Removing stopwords\n",
    "    stop_words = stopwords.words(\"english\") # 179 stopwords for english\n",
    "    dataframe['Text'] = dataframe['Text'].apply(lambda x : \" \".join(x for x in str(x).split() if x not in stop_words))\n",
    "    \n",
    "    # Apply the nouns_only functions to all the text in the dataframe\n",
    "    dataframe['Text'] = dataframe['Text'].apply(lambda x : adj_only(x))    \n",
    "    \n",
    "    # Counting the term frequency to figure out the most common features mentioned in the reviews\n",
    "\n",
    "    TF = dataframe[\"Text\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n",
    "    TF.columns = [\"Word\", \"TF\"]\n",
    "\n",
    "    # Sorting to get the most common words\n",
    "    TF = TF.sort_values(by = \"TF\", ascending = False)\n",
    "\n",
    "    # Getting 40 top common words\n",
    "    words = TF['Word'].head(60).reset_index(drop = True)\n",
    "    words = words.values.tolist()\n",
    "    \n",
    "    # Removing some meaningless words in the top 40\n",
    "    remove_val = ['','people','blocker','blocking','thing','something','place','ha','wa','browse','please',\n",
    "                  'doe','dont','None','cant','lot','im','doesnt','thanks','love','pc','way','day','none','everything',\n",
    "                  'ive','u','uc','app','give','download','opera','want','read','web','tab','website','thank', 'browser','dial',\n",
    "                  'desktop','device','ui','ea','due','le','screen','hello','address','wish','dear','seasoni','kkkkkk','wont','piece','hear']\n",
    "    for i in remove_val:\n",
    "        for word in words:\n",
    "            if word == i:\n",
    "                words.remove(i)\n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc2b18-cec5-4809-abb6-d018279f337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary consisting of multiple dataframes that contains \"word in words\"\n",
    "\n",
    "adict_df_adj = {}\n",
    "\n",
    "for word in words:\n",
    "    adict_df_adj[word] = df[df['Text'].str.lower().str.contains(word)]\n",
    "    adict_df_adj[word] = pd.DataFrame(adict_df_adj[word]['Text'])\n",
    "    \n",
    "    # Removing duplicated rows in each dataframe\n",
    "    adict_df_adj[word] = adict_df_adj[word].drop_duplicates(subset = ['Text']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2cc9f9-2cd0-4f8c-ae11-6a80e32f4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to save the most common words from each dataframe containing \"word in words\"\n",
    "\n",
    "adict_words_adjs = {}\n",
    "\n",
    "for word in words:\n",
    "    adict_words_adjs[word] = nlp_adj(adict_df_adj[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848dc62-598f-44d8-9d23-1183a7bc2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to save all essentially required columns \n",
    "\n",
    "sub_word = [] \n",
    "main_word = []\n",
    "text_col = []\n",
    "date_col = []\n",
    "id_col = []\n",
    "\n",
    "\n",
    "for i in range(len(df['Word'])):\n",
    "    for j in range(len(adict_words_adjs[(df['Word'][i])])):\n",
    "        if df['Word'][i] in df['Text'][i].lower() and (adict_words_adjs[df['Word'][i]][j]) in df['Text'][i].lower():\n",
    "            text_col.append(df['Text'][i])\n",
    "            main_word.append(df['Word'][i])\n",
    "            sub_word.append(adict_words_adjs[df['Word'][i]][j])\n",
    "            id_col.append(df['ID'][i])\n",
    "            date_col.append(df['Date'][i])\n",
    "            \n",
    "# Then save all the columns created above to a dictionary\n",
    "my_dict1 = {\"ID\" : id_col,\n",
    "           \"Date\" : date_col,\n",
    "            \"Word\" : main_word,\n",
    "           \"Sub_word\" : sub_word,\n",
    "           \"Text\" : text_col}\n",
    "\n",
    "# Convert the dictionary into a dataframe\n",
    "my_dict1 = pd.DataFrame(my_dict1)\n",
    "my_dict1 = my_dict1.drop_duplicates()\n",
    "\n",
    "# Removing rows that have the sub-word = main_word\n",
    "my_dict1 = my_dict1[my_dict1['Word'] != my_dict1['Sub_word']]\n",
    "\n",
    "my_dict1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57c50b-fa55-46ea-b6b8-f06426e7aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining my_dict1 with the original df (from STEP 3) to get full features\n",
    "\n",
    "df_final = pd.merge(df, my_dict1, on = \"ID\", how = \"inner\")\n",
    "df_final.drop_duplicates(inplace = True)\n",
    "df_final.reset_index(drop = True)\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c72e6-e94e-41c4-af2d-92bec0264512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "\n",
    "# Remove duplicated columns\n",
    "df_final.drop(['Date_x','Word_x','Text_x'], axis = 1, inplace = True)\n",
    "\n",
    "# Rename columns\n",
    "df_final.rename(columns = {'Word_y':'main_word',\n",
    "                           'Date_y' : 'date',\n",
    "                           'Text_y' : 'text'\n",
    "                          }, inplace = True)\n",
    "\n",
    "# Move \"Main_word\" and \"Sub_word\" columns to the first\n",
    "move_col1 = df_final.pop('date')\n",
    "df_final.insert(1,'date', move_col1)\n",
    "\n",
    "move_col2 = df_final.pop('main_word')\n",
    "df_final.insert(2,'main_word', move_col2)\n",
    "\n",
    "move_col3 = df_final.pop('Sub_word')\n",
    "df_final.insert(3,'sub_word', move_col3)\n",
    "\n",
    "# Convert all column names to lower string\n",
    "df_final.columns = df_final.columns.str.lower()\n",
    "\n",
    "# Remove duplicates\n",
    "df_final.drop_duplicates(inplace = True)\n",
    "df_final.reset_index(drop = True)\n",
    "\n",
    "print(df_final.shape) \n",
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619a450-4c0e-4bda-a96e-d8ec45982c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe to Google Big Query\n",
    "# Save a df to BQ\n",
    "df_final.to_gbq(\n",
    "    \"bi_playground.sentiment_analysis_table_sub_word_updated_xxxyyyzzz\", \"osp-bu-mobile\", if_exists=\"replace\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
